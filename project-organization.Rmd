---
title: "Best Practices for Project Organization in the Empirical Policy and Social Science Analhysis"
author: "Shiro Kuriwaki (kuriwaki@g.harvard.edu)"
date: '`r Sys.Date()`'
output: 
  html_vignette:
    keep_md: true
---


```{r, include = FALSE}
library(googlesheets)

```


> Saying [social science researchers] should spend more time thinking about the way they write code would be like telling a novelist that she should spend more time thinking about how best to use Microsoft Word.
> -- Gentzkow and Shapiro (2014)


This brief memo outlines one way to organize an empirical project in the Social Sciences, an area that is increasingly data-driven, program-driven, and collaborative. I rely most on the excellent writeup by economists Matthew Gentzkow and Jesse Shapiro, "Code and Data for the Social Sciences: A Practitioner’s Guide", as well as other resources that have established best practices. My intent is to summarize the key points and update with new resources, rather than re-invent the wheel.

(prepared for API 201-Z optional session )

## Motivation

Learning how to write on new apps may sound like an optional hobby that "just" makes things look nice (as the opening quote indicates). That is only partially true -- good organization and the use of appropriate tools improve the quality of your work, and more prominently, reduce the number of errors you make, thereby optimizing on your time.

The environment for data analysis and presentation is rapidly increasing. Only recently have we seen:
- Many organizations handling __more data__ at cheaper cost
- Many data analysis and version control tools becoming __open source__
- Online __collaboration__ becoming more prevalent
- Computer science and statistics attracting more students

and with these challenges come new tools to pick up.

Learning these tricks is an investment, but a little training on well-tested best practices can go a long way.


## Original Principles by Gentzkow and Shapiro

The recommendations by Gentzkow and Shapiro are central:

1. Automation
   * Automate everything that can be automated
  * Write a single script that executes all code from beginning to end
2. Version Control
   * Store code and data under version control
   * Run the whole directory before checking it back in
3. Directories (Folder Structure)
   * Separate directories by function
   * Separate files into inputs and outputs
   * Make directories portable
4. Keys
   * Store cleaned data in tables with unique, non-missing keys
   * Keep data normalized as far into your code pipeline as you can.
5. Abstraction
   * Abstract to eliminate redundancy.
   * Abstract to improve clarity.
   * Otherwise, don’t abstract.
6. Documentation
   * Don’t write documentation you will not maintain.
   * Code should be self-documenting.
7. Management
   * Manage tasks with a task management system.
   * E-mail is not a task management system.
8. Code style
   * Keep it short and purposeful
   * Make your functions shy
   * Order your functions for linear reading
   * Use descriptive names
   * Pay special attention to coding algebra
   * Make logical switches intuitive
   * Be consistent
   * Check for errors
   * Write tests
   * Profile slow code relentlessly.
   * Store “too much” output from slow code
   * Separate slow code from fast code


## Gentzkow and Shapiro,  modified

A version of the above tailored to those whose main job is not research, but data analysis.

## Priniciples

1. Automation
   i) Automate everything that can be automated
   ii) Each script should executes all code from beginning to end
   iii) Embed Figures and Tables to auto-update in your document

2. Abstraction
   i) Abstract to eliminate redundancy.
   ii) Abstract to improve clarity.
   iii) Otherwise, don’t abstract.

## Organization

3. Directories (Folder Structure)
   i) Separate directories by function
   ii) Separate files into inputs and outputs
   iii) Make directories portable

4. Datasets
   i) Store cleaned data in tables with unique, non-missing keys

5. Code organization
   i) One script for one step (build, analyze, figures)

## Management

6. Documentation
   i) Don’t write documentation you will not maintain.
   ii) Code should be self-documenting.

7. Management
   i) Manage tasks with a task management system.
   ii) E-mail is not _the best_ task management system: Git is the best, with a middle-ground with Dropbox Paper, Tasks in Google Docs, Slack.
   iii) Use a version control system

8. Open-sourcing
   i) Consider Open-sourcing your datasets, figures, or code

## Production
9. Writing Software
   i) Use an editor you have the most control over (preferably not WYSIWIG)

10. Figures and Tables

   i) One message per figure/table
   ii) Maximize the data-ink ratio
   iii) Drop unnnecessary digits
   iv) Modify aspect-ratios of figures

11. Code style
   i) Use descriptive names
   ii) Don't repeat code more than twice
   iii) Keep it short and purposeful
   iv) Make your functions shy
   v) Be consistent
   vi) Write tests

12. Typesetting
   i) Use generous margins
   ii) Use a consistent font


## Automation

   i) Automate everything that can be automated
   ii) Each script should executes all code from beginning to end
   iii) Embed Figures and Tables to auto-update in your document


You will be revising your analysis and writeup many more times than you anticipate. That's why short and automated code is essential. 

For example, these lines of code reads in data from Google Sheets, formats it, and loads the necessary packages, and saves a figure of a printable size. 

```{r, eval = FALSE}
library(googlesheets)

gapminder_sheet <- gs_gap()
df <- gs_read(gapminder_sheet, ws = "Africa", col_types = cols())

# look at data
glimpse(df)

df$log_gdppercap <- log(df$gdpPercap)

ggplot(df, aes(x = log_gdppercap, y = lifeExp, size = pop)) +
  facet_wrap(~year, nrow = 2) +
  geom_point() +
  geom_label(data = filter(df, country %in% "Rwanda"), aes(label = country)) +
  guides(size = FALSE) +
  labs(x = "log GDP per Capita", y = "Average Life Expectancy")

ggsave("figures/gapminder_africa.pdf", width = 5, height = 3)
```


Then, an text editor like Rmarkdown or LaTeX compiles a plain text file that embeds this Figure.
```{sh, eval = FALSE}
Project Report
============
Shiro Kuriwaki

# Introduction

# Data


# Analysis
![Rwanda's Civil War led to a substnantial Drop in its Life Expectancy](figures/gapminder_africa.pdf)

```


# Abstraction

   i) Abstract to eliminate redundancy.
   ii) Abstract to improve clarity.
   iii) Otherwise, don’t abstract.


It is also normal for you to try something out and then do another version of that. Suppose that, in analyzing Brazilian municipality data, we want to generate a new variable that computes the difference in a municipality's literacy level with its state-average. In Stata you would try

```{sh, eval = FALSE}
 egen mean_literate91 = mean(literate91), by(state)
 generate relative_lit_state = mean_literate91  - literate
```

and then look at a histogram,
```{sh, eval = FALSE}
histogram relative_lit_state
```


Intrigued, suppose you want to do the same type of calculation but with a different variable, the poverty rate. A redundant but common next step is to copy-paste the two lines of code and make sure to replace the relevant parts:


```{sh, eval = FALSE}
 egen mean_poverty91 = mean(poverty91), by(state)
 generate relative_pov_state = mean_poverty91  - poverty91
 histogram relative_pov_state
```


What about doing the same thing not by state, but by region? And so on and so on.. you end up with lines of code that is redundant in terms of the key operation (compute means, subtract the mean from the value); only the variables have changed. After a while, you get tired of copy-pasting and tediously editing so you stop analyzing the data and move out. Even worse, you make a copy-paste error and forget to swap a variable. 

Here's where the abstraction via defining your own function becomes important:
```{sh, eval = FALSE}
program hist_relative_rate
         syntax, invar(varname) outvar(name) byvar(varname)
         tempvar mean_invar
         egen `mean_invar'= mean(`invar'), by(`byvar')
         gen `outvar' = `mean_invar' - `invar'
         histogram `outvar'
end
```

Now we can do the same function with only one line of code!
```{sh, eval = FALSE}
hist_relative_rate, invar(poverty80) outvar(relative_pov80_state) byvar(state)
```

Moreover, repeating the same procedure with different variables requires you only write as many lines as there are different specifications, rather than having the extra `egen`. 

```{r, eval = FALSE}
hist_relative_rate, invar(poverty80) outvar(relative_pov80_state) byvar(state)
hist_relative_rate, invar(poverty91) outvar(relative_pov91_state) byvar(state)
hist_relative_rate, invar(poverty80) outvar(relative_pov80_region) byvar(region)
```


Not only is this fewer keystrokes, it is much more **readable**. It is clear what the function `hist_relative_rate` is essentially doing, whereas understanding three lines takes more time than people have time for. We'll see later that adding comments to this code is also not a great idea, because that documentation can quickly become deprecated.